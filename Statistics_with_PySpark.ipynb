{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPBaf5cFthQDwWVjR6mrCQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Statistics-with-Python/blob/main/Statistics_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PySpark to calculate Summary Statistics\n",
        "\n",
        "PySpark Python APIs<br>\n",
        "PySpark provides a Python library called pyspark that exposes the Spark functionality through Python APIs. This library allows developers to interact with Spark's core components, such as SparkContext, DataFrame, and RDD, using Python code.<br>\n",
        "The pyspark library provides a high-level interface that abstracts away the complexities of distributed computing, allowing Python developers to focus on data analysis and manipulation tasks.<br>\n",
        "PySpark supports both the interactive PySpark shell (pyspark) and Python script execution using the spark-submit command.<br>\n"
      ],
      "metadata": {
        "id": "StJfv9sxPDuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "K9MoGPiJPh-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO8e3zr4O_Zz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "        ['A', 'East', 8, 9],\n",
        "        ['A', 'East', 10, 3],\n",
        "        ['B', 'West', 6, 12],\n",
        "        ['B', 'West', 6, 4],\n",
        "        ['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate summary statistics for all columns**"
      ],
      "metadata": {
        "id": "KbBTIZtCyayy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for the summary command is:\n",
        "\n",
        "count: The number of values in the column<br>\n",
        "mean: The mean valuebr>\n",
        "stddev: The standard deviation of values<br>\n",
        "min: The minimum value<br>\n",
        "25%: The 25th percentile <br>\n",
        "50%:The 50th percentile (this is also the median)<br>\n",
        "75%: The 75th percentile<br>\n",
        "max: The max value<br>"
      ],
      "metadata": {
        "id": "6tVQpEpQyqb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary().show()"
      ],
      "metadata": {
        "id": "Mu3hP3lAya9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary statistics for numeric columns.**"
      ],
      "metadata": {
        "id": "HcwWn0HkzByX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#identify numeric columns in DataFrame\n",
        "numeric_cols = [c for c, t in df.dtypes if t.startswith('string')==False]\n",
        "\n",
        "#calculate summary statistics for only the numeric columns\n",
        "df.select(*numeric_cols).summary().show()"
      ],
      "metadata": {
        "id": "vhEU6RPgzB9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, min, max\n",
        "df.select([mean('points'), min('points'), max('points')]).show()"
      ],
      "metadata": {
        "id": "AePj7vZU-r7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measuring Covariance**<br>\n",
        "Covariance is a measure of how two variables change with respect to each other.<br>\n",
        "\n",
        "A positive number would mean that there is a tendency that as one variable increases, the other increases as well.<br>\n",
        "\n",
        "A negative number would mean that as one variable increases, the other variable has a tendency to decrease.<br>"
      ],
      "metadata": {
        "id": "35LsILtM-7Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "df.stat.cov('points', 'assists')"
      ],
      "metadata": {
        "id": "Hy-QQvaJ-7WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Tabulation**<br>\n",
        "\n",
        "Cross Tabulation provides a table of the frequency distribution for a set of variables. <br>\n",
        "\n",
        "Cross-tabulation is a powerful tool in statistics that is used to observe the statistical significance (or independence) of variables."
      ],
      "metadata": {
        "id": "Teb7gwKC_uSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with two columns (name, item)\n",
        "names = [\"Alice\", \"Bob\", \"Mike\", \"Toby\"]\n",
        "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\", \"walnuts\"]\n",
        "\n",
        "df = spark.createDataFrame([(names[i % 4], items[i % 6]) for i in range(100)], [\"name\", \"item\"])\n",
        "\n",
        "# Take a look at the first 10 rows.\n",
        "df.show(10)"
      ],
      "metadata": {
        "id": "iuKx9VtG_ubP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cardinality of columns we run crosstab on cannot be too big. <br>\n",
        "\n",
        "The number of distinct “name” and “item” cannot be too large. Just imagine if “item” contains 1 billion distinct entries: how would you fit that table on your screen?"
      ],
      "metadata": {
        "id": "34GSWvfyA4hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.stat.crosstab(\"name\", \"item\").show()"
      ],
      "metadata": {
        "id": "6bZiszHwAfpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequent Items**<br>\n",
        "\n",
        "Figuring out which items are frequent in each column can be very useful to understand a dataset.\n",
        "\n",
        "PySpark implemented a one-pass algorithm proposed by Karp et al. This is a fast, approximate algorithm that always returns all the frequent items that appear in a user-specified minimum proportion of rows.<br>\n",
        "\n",
        "**Note**: that the result might contain false positives, i.e. items that are not frequent."
      ],
      "metadata": {
        "id": "zm2w9SX0A_bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
        "\n",
        "df.show(10)"
      ],
      "metadata": {
        "id": "7Fsb-jnmBAnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the above DataFrame, the following code finds the frequent items that show up 40% of the time for each column:"
      ],
      "metadata": {
        "id": "QqG6oPWUCJLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4) #40%\n",
        "\n",
        "freq.collect()[0]\n",
        "#Column a: 1 appeared 99 times"
      ],
      "metadata": {
        "id": "Cyvol7-bCJa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequent items for column combinations**<br>\n",
        "\n",
        "The combination of “a=99 and b=198”, and “a=1 and b=2” appear frequently in this dataset. Note that “a=11 and b=22” is a false positive.\n"
      ],
      "metadata": {
        "id": "iZhzirZ1DFWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import struct\n",
        "\n",
        "freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n",
        "\n",
        "freq.collect()[0]"
      ],
      "metadata": {
        "id": "AOFIleK0DFhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}